INFO - PPO - Running command 'ppo_run'
INFO - PPO - Started run with ID "48"
Creating env with params {'RUN_TYPE': 'ppo', 'SEEDS': [9456, 1887, 5578, 5987, 516], 'LOCAL_TESTING': False, 'EX_NAME': 'dispenser_side_specific_ppo_bc_train_random0_test2', 'SAVE_DIR': 'data/ppo_runs/2021_08_16-18_57_39_dispenser_side_specific_ppo_bc_train_random0_test2/', 'GPU_ID': 0, 'PPO_RUN_TOT_TIMESTEPS': 9000000.0, 'mdp_params': {'layout_name': 'random0', 'start_order_list': None, 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}}, 'env_params': {'horizon': 400}, 'mdp_generation_params': {'padded_mdp_shape': [11, 7], 'mdp_shape_fn': [[5, 11], [5, 7]], 'prop_empty_fn': [0.6, 1], 'prop_feats_fn': [0, 0.6]}, 'ENTROPY': 0.1, 'GAMMA': 0.99, 'sim_threads': 30, 'TOTAL_BATCH_SIZE': 12000, 'BATCH_SIZE': 400, 'MAX_GRAD_NORM': 0.1, 'LR': 0.0015, 'LR_ANNEALING': 2, 'VF_COEF': 0.1, 'STEPS_PER_UPDATE': 8, 'MINIBATCHES': 15, 'CLIPPING': 0.05, 'LAM': 0.98, 'SELF_PLAY_HORIZON': None, 'REW_SHAPING_HORIZON': 4000000.0, 'OTHER_AGENT_TYPE': 'bc_train', 'HM_PARAMS': [True, 0.3], 'NUM_HIDDEN_LAYERS': 3, 'SIZE_HIDDEN_LAYERS': 64, 'NUM_FILTERS': 25, 'NUM_CONV_LAYERS': 3, 'NETWORK_TYPE': 'conv_and_mlp', 'SAVE_BEST_THRESH': 50, 'TRAJECTORY_SELF_PLAY': True, 'VIZ_FREQUENCY': 50, 'grad_updates_per_agent': 90000.0}
Computing MediumLevelPlanner to be saved in /home/mzhao2/overcooked-teaming/overcooked_ai/overcooked_ai_py/data/planners/random0_am.pkl
It took 0.033466339111328125 seconds to create mlp
LOADING BC MODEL FROM: random0_bc_train_seed0
Loading a model without an environment, this model cannot be trained until it has a valid environment.
WARNING:tensorflow:From /home/mzhao2/anaconda3/envs/harl-gpu/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING - tensorflow - From /home/mzhao2/anaconda3/envs/harl-gpu/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /home/mzhao2/overcooked-teaming/stable-baselines/stable_baselines/common/policies.py:436: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING - tensorflow - From /home/mzhao2/overcooked-teaming/stable-baselines/stable_baselines/common/policies.py:436: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /home/mzhao2/anaconda3/envs/harl-gpu/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
WARNING - tensorflow - From /home/mzhao2/anaconda3/envs/harl-gpu/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
WARNING:tensorflow:From /home/mzhao2/anaconda3/envs/harl-gpu/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
WARNING - tensorflow - From /home/mzhao2/anaconda3/envs/harl-gpu/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
Mlp with different params or mdp found, computing from scratch
Computing MediumLevelPlanner to be saved in /home/mzhao2/overcooked-teaming/overcooked_ai/overcooked_ai_py/data/planners/random0_am.pkl
It took 0.03510570526123047 seconds to create mlp
NETWORK TYPE conv_and_mlp



Network conv_and_mlp 



WARNING:tensorflow:From /home/mzhao2/overcooked-teaming/baselines/baselines/common/input.py:57: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
WARNING - tensorflow - From /home/mzhao2/overcooked-teaming/baselines/baselines/common/input.py:57: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
(30, 5, 5, 20)
WARNING:tensorflow:From ../../human_aware_rl/baselines_utils.py:127: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.conv2d instead.
WARNING - tensorflow - From ../../human_aware_rl/baselines_utils.py:127: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.conv2d instead.
WARNING:tensorflow:From ../../human_aware_rl/baselines_utils.py:143: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING - tensorflow - From ../../human_aware_rl/baselines_utils.py:143: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
Last layer conv network output shape (30, 64)
(800, 5, 5, 20)
Last layer conv network output shape (800, 64)
TOT NUM UPDATES 0



Network conv_and_mlp 



TOT NUM UPDATES 750
SP envs: 0/30
Other agent actions took 4.260776996612549 seconds
Total simulation time for 400 steps: 7.159599781036377 	 Other agent action time: 0 	 55.86904467194933 steps/s
Curr learning rate 0.0015 	 Curr reward per step 0.01825

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8:  20%|██        | 3/15 [00:00<00:00, 29.10it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 87.47it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 178.29it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 166.54it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 173.68it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 173.14it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 176.67it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 169.23it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 175.77it/s]
Logging to /tmp/openai-2021-08-16-18-57-55-439676
--------------------------------------
| approxkl           | 0.0006857252  |
| clipfrac           | 0.055749997   |
| eplenmean          | 400           |
| eprewmean          | 7.3           |
| explained_variance | 0.00443       |
| fps                | 1504          |
| nupdates           | 1             |
| policy_entropy     | 1.791086      |
| policy_loss        | -0.0016412463 |
| serial_timesteps   | 400           |
| time_elapsed       | 7.97          |
| time_remaining     | 99.5          |
| total_timesteps    | 12000         |
| true_eprew         | 0             |
| value_loss         | 0.60193336    |
--------------------------------------
Current reward shaping 0.997
SP envs: 0/30
Other agent actions took 4.447277307510376 seconds
Total simulation time for 400 steps: 6.923017263412476 	 Other agent action time: 0 	 57.77827568247794 steps/s
Curr learning rate 0.001499 	 Curr reward per step 0.018943000000000005

0/8:   0%|          | 0/15 [00:00<?, ?it/s]
0/8: 100%|██████████| 15/15 [00:00<00:00, 164.09it/s]

1/8:   0%|          | 0/15 [00:00<?, ?it/s]
1/8: 100%|██████████| 15/15 [00:00<00:00, 166.37it/s]

2/8:   0%|          | 0/15 [00:00<?, ?it/s]
2/8: 100%|██████████| 15/15 [00:00<00:00, 166.43it/s]

3/8:   0%|          | 0/15 [00:00<?, ?it/s]
3/8: 100%|██████████| 15/15 [00:00<00:00, 170.36it/s]

4/8:   0%|          | 0/15 [00:00<?, ?it/s]
4/8: 100%|██████████| 15/15 [00:00<00:00, 170.93it/s]

5/8:   0%|          | 0/15 [00:00<?, ?it/s]
5/8: 100%|██████████| 15/15 [00:00<00:00, 173.42it/s]

6/8:   0%|          | 0/15 [00:00<?, ?it/s]
6/8: 100%|██████████| 15/15 [00:00<00:00, 167.16it/s]

7/8:   0%|          | 0/15 [00:00<?, ?it/s]
7/8: 100%|██████████| 15/15 [00:00<00:00, 165.35it/s]
-------------------------------------
| approxkl           | 0.0005903885 |
| clipfrac           | 0.07159373   |
| eplenmean          | 400          |
| eprewmean          | 7.44         |
| explained_variance | 0.00207      |
| fps                | 1565         |
| nupdates           | 2            |
| policy_entropy     | 1.7895424    |
| policy_loss        | -0.002372528 |
| serial_timesteps   | 800          |
| time_elapsed       | 15.6         |
| time_remaining     | 97.5         |
| total_timesteps    | 24000        |
| true_eprew         | 0            |
| value_loss         | 0.6069453    |
-------------------------------------
Current reward shaping 0.994
SP envs: 0/30
